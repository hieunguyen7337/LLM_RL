{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HmS6NCR9biaw"
      ],
      "authorship_tag": "ABX9TyODQpgKEHmHi/cY/WKUxtb3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hieunguyen7337/LLM_RL/blob/main/Hangman_game_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "uQ46C0hlEShX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorWithPadding\n",
        "import gc\n",
        "import random\n",
        "import json\n",
        "import torch\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "ougOkRqKW24P"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hieunguyen7337/LLM_RL.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tgka96imQy8G",
        "outputId": "38302d0a-7393-4144-a7f9-5d74d9dc6612"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLM_RL'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 76 (delta 33), reused 12 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (76/76), 457.83 KiB | 1.09 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57cbffef"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting"
      ],
      "metadata": {
        "id": "wW69ksh4Ty1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Settings (safe for T4 or CPU)\n",
        "# ---------------------------\n",
        "# MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
        "# MODEL_NAME = \"Qwen/Qwen3-8B\"\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
        "# MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "# MODEL_NAME = \"google/gemma-3-270m\"\n",
        "# MODEL_NAME       = \"facebook/opt-350m\"   # FP32 on T4: start small; try opt-1.3b if VRAM allows\n",
        "DATA_PATH        = \"/content/LLM_RL/hangman_dataset.json\"\n",
        "BATCH_SIZE       = 512                    # raise/lower based on VRAM; FP32 uses ~2x memory vs FP16\n",
        "MAX_NEW_TOKENS   = 1\n",
        "TRUNCATE_TO      = 2048                  # shorten if memory-bound\n",
        "PAD_TO_MULTIPLE  = 8                     # helps kernel efficiency; fine in FP32\n",
        "NUM_WORKERS      = 0                     # dataloader workers"
      ],
      "metadata": {
        "id": "Yz1vJjpuTx-M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy-first: disable TF32 (not relevant on T4, but safe everywhere)\n",
        "torch.backends.cuda.matmul.allow_tf32 = False\n",
        "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
        "    torch.set_float32_matmul_precision(\"highest\")  # accuracy-preferring\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE  = torch.float32  # force full precision"
      ],
      "metadata": {
        "id": "ehCu65YbUrf-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Random word list"
      ],
      "metadata": {
        "id": "2yF_nG75iA3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "id": "1iupSIWrioPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "\n",
        "fake = Faker()"
      ],
      "metadata": {
        "id": "lOmVgmxBinNA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake.word()"
      ],
      "metadata": {
        "id": "ZQg2x_pJitR8",
        "outputId": "84131ad8-3143-46ac-d54d-b7cef0770130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'town'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/LLM_RL/testing_random_words_list.txt', 'r') as f:\n",
        "  word_list = f.readlines()\n",
        "\n",
        "# Remove newline characters\n",
        "word_list = [word.strip() for word in word_list]"
      ],
      "metadata": {
        "id": "WytoLbi6R2Mf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_words = []\n",
        "while len(random_words) < 500:\n",
        "    if len(random_words) % 100 == 0:\n",
        "        print(f\"Generated {len(random_words)} words\")\n",
        "    w = fake.word()\n",
        "    if w not in word_list and w not in random_words:\n",
        "        random_words.append(w)"
      ],
      "metadata": {
        "id": "71u_gAbQhcU6",
        "outputId": "b02a4c72-d961-40b2-faf4-da678145aebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 0 words\n",
            "Generated 100 words\n",
            "Generated 100 words\n",
            "Generated 100 words\n",
            "Generated 200 words\n",
            "Generated 200 words\n",
            "Generated 200 words\n",
            "Generated 300 words\n",
            "Generated 300 words\n",
            "Generated 300 words\n",
            "Generated 300 words\n",
            "Generated 400 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_words[24:31]"
      ],
      "metadata": {
        "id": "mR1TPHMpiLnw",
        "outputId": "61438184-8d03-4520-f20d-a235436e0d7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['within', 'represent', 'method', 'know', 'guy', 'become', 'whatever']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"training_random_words_list.txt\"\n",
        "\n",
        "with open(file_path, \"w\") as f:\n",
        "    for word in random_words:\n",
        "        f.write(word + \"\\n\")"
      ],
      "metadata": {
        "id": "wl5BevZHi-QW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess Data"
      ],
      "metadata": {
        "id": "N4QgzH71DJbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json"
      ],
      "metadata": {
        "id": "vWzt--Xxmwhs"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/LLM_RL/testing_random_words_list.txt', 'r') as f:\n",
        "  word_list = f.readlines()\n",
        "\n",
        "# Remove newline characters\n",
        "word_list = [word.strip() for word in word_list]"
      ],
      "metadata": {
        "id": "uh8kX_ych-ez"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nNumber of words: {len(word_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2e3kulhYoNr",
        "outputId": "1150facb-c150-41a5-dd7a-a11f3a551436"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of words: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate words\n",
        "if len(word_list) == len(set(word_list)):\n",
        "  print(\"No duplicate words found in the list.\")\n",
        "else:\n",
        "  print(\"Duplicate words found in the list.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lY5pkjtYKRH",
        "outputId": "1bff924b-bc15-4755-a785-e0f4b201f4d9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No duplicate words found in the list.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove words with hyphens\n",
        "word_list = [word for word in word_list if '-' not in word and ' ' not in word]\n",
        "\n",
        "# Check the number of words remaining\n",
        "print(f\"\\nNumber of words remaining after removing hyphens: {len(word_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGmRRdCCYUTB",
        "outputId": "23cc5465-fbef-4993-9144-4bd5407d7c45"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of words remaining after removing hyphens: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get a specified number of random words (e.g., 5)\n",
        "# # num_random_words = 200\n",
        "# num_random_words = 2\n",
        "# random_words = random.sample(word_list, num_random_words)\n",
        "# # print(f\"\\n{num_random_words} random words from the list\")"
      ],
      "metadata": {
        "id": "RZm7-SdoSRyL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_words = word_list"
      ],
      "metadata": {
        "id": "0tsYcOLlbI2P"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list to store the input texts\n",
        "hangman_dataset = []\n",
        "input_texts = []\n",
        "\n",
        "for secret_word in random_words:\n",
        "  for i in range(5):\n",
        "    word_length = len(secret_word)\n",
        "    current_state = [\"_\" for _ in range(word_length)]\n",
        "    incorrect_guesses_remaining = 6\n",
        "    guessed_letters = []\n",
        "\n",
        "    # Simulate some random guesses to create a game in progress\n",
        "    num_simulated_guesses = random.randint(0, len(secret_word)) # Simulate 0 to 3 guesses\n",
        "    # num_simulated_guesses = 20\n",
        "    available_letters = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "    correct_letters = list(set(secret_word))\n",
        "\n",
        "    for _ in range(num_simulated_guesses):\n",
        "      if random.random() > 0.5:\n",
        "        guess = random.choice(available_letters)\n",
        "      else:\n",
        "        guess = random.choice(correct_letters)\n",
        "\n",
        "      correct_letters.remove(guess) if guess in correct_letters else None\n",
        "      available_letters.remove(guess) if guess in available_letters else None\n",
        "\n",
        "      guessed_letters.append(guess.upper())\n",
        "\n",
        "      if guess in secret_word:\n",
        "        for i in range(word_length):\n",
        "          if secret_word[i] == guess:\n",
        "            current_state[i] = guess\n",
        "\n",
        "      else:\n",
        "        incorrect_guesses_remaining -= 1\n",
        "        if incorrect_guesses_remaining == 0:\n",
        "          break # Game over during simulation\n",
        "\n",
        "      if len(correct_letters) == 0:\n",
        "        break\n",
        "\n",
        "    # Create the input text for the prompt\n",
        "    input_text = f\"\"\"You are playing a game of Hangman.\n",
        "\n",
        "Your task is to guess a single character.\n",
        "\n",
        "The word has a certain number of letters.\n",
        "The current state of the word is shown with guessed letters filled in and blanks for the unknown letters.\n",
        "The number of incorrect guesses remaining is listed.\n",
        "All letters that have been guessed so far are listed.\n",
        "\n",
        "You will format your response as a single uppercase letter at the end\n",
        "\n",
        "The word has {word_length} letters.\n",
        "The current state is: {' '.join(current_state)}\n",
        "Incorrect guesses remaining: {incorrect_guesses_remaining}\n",
        "Guessed letters: {guessed_letters}\n",
        "\n",
        "Correct response:\"\"\"\n",
        "    # if True:\n",
        "    if incorrect_guesses_remaining > 0 and len(correct_letters) > 0 and input_text not in input_texts:\n",
        "      hangman_dataset.append({\"prompt\": input_text, \"word\": secret_word})\n",
        "      input_texts.append(input_text)"
      ],
      "metadata": {
        "id": "vTY8iFkFUKDI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first generated input text to verify\n",
        "print(len(hangman_dataset))\n",
        "print()\n",
        "i = 2\n",
        "print(hangman_dataset[i][\"prompt\"])\n",
        "print(hangman_dataset[i][\"word\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oU381QCX9WK",
        "outputId": "051a1500-6140-4d6c-f7f0-782e95fe3032"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1825\n",
            "\n",
            "You are playing a game of Hangman.\n",
            "\n",
            "Your task is to guess a single character.\n",
            "\n",
            "The word has a certain number of letters.\n",
            "The current state of the word is shown with guessed letters filled in and blanks for the unknown letters.\n",
            "The number of incorrect guesses remaining is listed.\n",
            "All letters that have been guessed so far are listed.\n",
            "\n",
            "You will format your response as a single uppercase letter at the end\n",
            "\n",
            "The word has 5 letters.\n",
            "The current state is: _ b _ _ _\n",
            "Incorrect guesses remaining: 6\n",
            "Guessed letters: ['B']\n",
            "\n",
            "Correct response:\n",
            "above\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save both lists in one file\n",
        "with open(\"training_hangman_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(hangman_dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "wkNCIeEP8TyU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Preprocessed Data"
      ],
      "metadata": {
        "id": "LzoHv2W1DcAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON file\n",
        "with open(\"/content/LLM_RL/hangman_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    hangman_dataset = json.load(f)"
      ],
      "metadata": {
        "id": "eNrpTrlCQpWA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(hangman_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWILvmDfQrmt",
        "outputId": "d000bad0-c80e-493d-dda9-986e3d444c18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10429"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 3\n",
        "print(hangman_dataset[i][\"prompt\"])\n",
        "print(hangman_dataset[i][\"word\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfeHcVClFIZJ",
        "outputId": "826564c9-ecc7-4a23-9278-e50a67b985dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are playing a game of Hangman.\n",
            "\n",
            "Your task is to guess a single character.\n",
            "\n",
            "The word has a certain number of letters.\n",
            "The current state of the word is shown with guessed letters filled in and blanks for the unknown letters.\n",
            "The number of incorrect guesses remaining is listed.\n",
            "All letters that have been guessed so far are listed.\n",
            "\n",
            "You will format your response as a single uppercase letter at the end\n",
            "\n",
            "The word has 8 letters.\n",
            "The current state is: i _ _ o _ _ _ _\n",
            "Incorrect guesses remaining: 2\n",
            "Guessed letters: ['S', 'W', 'I', 'Z', 'O', 'H']\n",
            "\n",
            "Correct response:\n",
            "imported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Load dataset (JSON array or JSONL)\n",
        "# ---------------------------\n",
        "ds = load_dataset(\"json\", data_files=DATA_PATH)[\"train\"]\n",
        "PROMPT_COL = \"prompt\""
      ],
      "metadata": {
        "id": "Y33hJhQZUvqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load model"
      ],
      "metadata": {
        "id": "E8PXkYRDC43V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Tokenizer & model (FP32)\n",
        "# ---------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "tokenizer.padding_side = \"left\"\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "F3692SKhU3J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=DTYPE,     # full precision\n",
        "    device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
        ")\n",
        "if DEVICE == \"cpu\":\n",
        "    model.to(DEVICE)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "N7Gli3e2U4hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional compile (doesn't change numerics)\n",
        "try:\n",
        "    if DEVICE == \"cuda\":\n",
        "        model = torch.compile(model, mode=\"max-autotune\")\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "id": "wV5yXcZAU9pR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Pre-tokenize (no padding here; collator handles it dynamically)\n",
        "# ---------------------------\n",
        "def tok(batch):\n",
        "    return tokenizer(\n",
        "        batch[PROMPT_COL],\n",
        "        truncation=True,\n",
        "        max_length=TRUNCATE_TO,\n",
        "        padding=False,\n",
        "        return_attention_mask=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "Xds2Y7_mVEB7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference Model"
      ],
      "metadata": {
        "id": "P2fA2doGD0Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: drop original string columns so the collator only sees tensors\n",
        "tokenized = ds.map(tok, batched=True, remove_columns=ds.column_names)"
      ],
      "metadata": {
        "id": "Tnf046PDVJNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collator = DataCollatorWithPadding(\n",
        "    tokenizer=tokenizer,\n",
        "    pad_to_multiple_of=PAD_TO_MULTIPLE if DEVICE == \"cuda\" else None,\n",
        "    return_tensors=\"pt\",\n",
        ")"
      ],
      "metadata": {
        "id": "V6t0CdkJVRGC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DataLoader(\n",
        "    tokenized,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=(DEVICE == \"cuda\"),\n",
        "    persistent_workers=NUM_WORKERS > 0,\n",
        "    collate_fn=collator,\n",
        ")"
      ],
      "metadata": {
        "id": "0zlpv1KfVVcx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Generation (deterministic & fast)\n",
        "# ---------------------------\n",
        "gen_kwargs = dict(\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    do_sample=False,  # greedy = deterministic & faster\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    use_cache=True,\n",
        ")"
      ],
      "metadata": {
        "id": "iXonEEU3VYr4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_generations = []\n",
        "with torch.inference_mode():\n",
        "    pbar = tqdm(total=len(loader.dataset), desc=\"Generating\", unit=\"ex\")\n",
        "    for batch in loader:\n",
        "        input_ids = batch[\"input_ids\"].to(DEVICE, non_blocking=True)\n",
        "        attention_mask = batch[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
        "\n",
        "        out = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict_in_generate=True,   # <-- add\n",
        "            output_scores=False,            # <-- optional\n",
        "            **gen_kwargs,\n",
        "        )\n",
        "        seqs = out.sequences                                   # [B, in_len + new_len]\n",
        "        new_token_ids = seqs[:, input_ids.shape[1]:]           # <-- slice by true input length\n",
        "        texts = tokenizer.batch_decode(\n",
        "            new_token_ids,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=True,\n",
        "        )\n",
        "        all_generations.extend([t.strip() for t in texts])\n",
        "        pbar.update(len(texts))\n",
        "\n",
        "        # ---- memory cleanup per batch ----\n",
        "        del batch, input_ids, attention_mask, out, new_token_ids, texts\n",
        "        if DEVICE == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.ipc_collect()\n",
        "        gc.collect()\n",
        "\n",
        "    pbar.close()"
      ],
      "metadata": {
        "id": "TplS_L7Tg-fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_generations"
      ],
      "metadata": {
        "id": "_8aaSEz9bLuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attach to dataset and peek\n",
        "ds = ds.add_column(\"model_output\", all_generations)\n",
        "print(ds.select(range(min(5, len(ds)))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAsxnq45VkGm",
        "outputId": "29ee4cd8-0097-4ec3-e2bc-7d140639cd27"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['prompt', 'word', 'model_output'],\n",
            "    num_rows: 5\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) Save results\n",
        "ds.to_json(\"data_with_outputs_\" + MODEL_NAME.split(\"/\")[1] + \".jsonl\", orient=\"records\", lines=True)"
      ],
      "metadata": {
        "id": "1mM5nakCVlDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "zL3tRfszx8k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "Y7C67E6L4j3a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty list to store the parsed JSON objects\n",
        "out_Qwen2_5_0_5B_Instruct = []\n",
        "out_Qwen2_5_0_5B = []\n",
        "\n",
        "# Open the .jsonl file and read each line\n",
        "with open(\"/content/data_with_outputs_1_Qwen2.5-0.5B-Instruct.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # For each line, parse the JSON object and append it to the list\n",
        "        out_Qwen2_5_0_5B_Instruct.append(json.loads(line))\n",
        "\n",
        "with open(\"/content/data_with_outputs_1_Qwen2.5-0.5B.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        # For each line, parse the JSON object and append it to the list\n",
        "        out_Qwen2_5_0_5B.append(json.loads(line))"
      ],
      "metadata": {
        "id": "kaXcxcnb4xmr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(out_Qwen2_5_0_5B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMoPy8bH_cur",
        "outputId": "4f49ade4-8260-43b2-dd3b-5e2f3d868edc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10429"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, ast\n",
        "from typing import List, Dict, Any"
      ],
      "metadata": {
        "id": "kND69yb1DAmH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- helpers to parse prompt fields ---\n",
        "def parse_prompt(prompt: str):\n",
        "    \"\"\"\n",
        "    Returns (guessed_letters_upper: List[str], state_tokens_upper: List[str])\n",
        "    where state tokens are letters or '_' from the 'current state' line.\n",
        "    \"\"\"\n",
        "    guessed_letters = []\n",
        "    m = re.search(r\"Guessed letters:\\s*(\\[[^\\]]*\\])\", prompt)\n",
        "    if m:\n",
        "        try:\n",
        "            guessed_letters = [s.upper() for s in ast.literal_eval(m.group(1)) if isinstance(s, str)]\n",
        "        except Exception:\n",
        "            guessed_letters = []\n",
        "    guessed_letters = list(dict.fromkeys(guessed_letters))  # dedupe, keep order\n",
        "\n",
        "    state_tokens = []\n",
        "    m2 = re.search(r\"The current state is:\\s*([A-Za-z_ ]+)\", prompt)\n",
        "    if m2:\n",
        "        state_line = m2.group(1)\n",
        "        state_tokens = state_line.split()  # e.g. [\"_\", \"_\", \"P\", \"O\", \"_\", \"_\", \"E\", \"_\"]\n",
        "\n",
        "    state_tokens_upper = [t.upper() for t in state_tokens]\n",
        "    return guessed_letters, state_tokens_upper\n",
        "\n",
        "def evaluate_entry(entry: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evaluate one record: expects keys 'prompt', 'word', 'model_output'.\n",
        "    Returns a dict with is_correct, error_types, and extra diagnostics.\n",
        "    \"\"\"\n",
        "    prompt = entry.get(\"prompt\", \"\")\n",
        "    word = str(entry.get(\"word\", \"\")).upper()\n",
        "    guess_raw = str(entry.get(\"model_output\", \"\")).strip()\n",
        "\n",
        "    errors = []\n",
        "    valid = True\n",
        "\n",
        "    # --- format checks ---\n",
        "    if len(guess_raw) == 0:\n",
        "        errors.append(\"Guess empty\")\n",
        "        valid = False\n",
        "    elif len(guess_raw) != 1:\n",
        "        errors.append(\"Guess is not a single character\")\n",
        "        valid = False\n",
        "\n",
        "    guess = guess_raw if len(guess_raw) == 1 else None\n",
        "    if guess is not None:\n",
        "        if not guess.isalpha():\n",
        "            errors.append(\"Non-alphabetic guess\")\n",
        "            valid = False\n",
        "\n",
        "    guessed_letters, state_tokens = parse_prompt(prompt)\n",
        "\n",
        "    # --- logical checks against game state ---\n",
        "    # Determine if guess reveals at least one new position\n",
        "    reveals_new = False\n",
        "    in_word = False\n",
        "    if guess is not None and guess.isalpha():\n",
        "        in_word = guess.upper() in set(word)\n",
        "\n",
        "        # Compare to current state; if any '_' position matches the guess in the target word -> reveals_new\n",
        "        L = min(len(state_tokens), len(word))\n",
        "        for i in range(L):\n",
        "            if state_tokens[i] == \"_\" and word[i] == guess.upper():\n",
        "                reveals_new = True\n",
        "                break\n",
        "        # If prompt state is shorter than word (edge case), consider remaining as hidden\n",
        "        if not reveals_new and len(state_tokens) < len(word):\n",
        "            for i in range(len(state_tokens), len(word)):\n",
        "                if word[i] == guess.upper():\n",
        "                    reveals_new = True\n",
        "                    break\n",
        "\n",
        "        # Already guessed?\n",
        "        if guess.upper() in guessed_letters:\n",
        "            errors.append(\"Already guessed\")\n",
        "        # Wrong letter?\n",
        "        elif guess.upper() not in set(word):\n",
        "            errors.append(\"Guess is not a character in word\")\n",
        "\n",
        "    is_correct = valid and (guess is not None) and in_word and (guess.upper() not in guessed_letters) and reveals_new\n",
        "\n",
        "    return {\n",
        "        \"word\": entry.get(\"word\"),\n",
        "        \"guess_raw\": guess_raw,\n",
        "        \"guess\": guess if guess is not None else \"\",\n",
        "        \"guessed_letters_in_prompt\": guessed_letters,\n",
        "        \"state_tokens_in_prompt\": state_tokens,\n",
        "        \"is_correct\": bool(is_correct),\n",
        "        \"error_types\": errors if errors else [\"Correct guess\"],\n",
        "    }\n",
        "\n",
        "# --- evaluate a list of records (e.g., ds.to_list()) ---\n",
        "def evaluate_records(records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    return [evaluate_entry(r) for r in records]"
      ],
      "metadata": {
        "id": "71vr4t3iASdk"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluate_records(out_Qwen2_5_0_5B_Instruct)\n",
        "# results = evaluate_records(out_Qwen2_5_0_5B)"
      ],
      "metadata": {
        "id": "moGn2iH7DC-9"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results[:3]"
      ],
      "metadata": {
        "id": "-Mqt0_jzFquJ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# out_Qwen2_5_0_5B[:3]"
      ],
      "metadata": {
        "id": "wmb9I_HeKKyX"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error = {}\n",
        "for r in results:\n",
        "  if r[\"error_types\"][0] not in error and len(r[\"error_types\"]) != 0:\n",
        "    error[r[\"error_types\"][0]] = 1\n",
        "  else:\n",
        "    error[r[\"error_types\"][0]] += 1"
      ],
      "metadata": {
        "id": "CLtPtKVaJoCa"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3r75vL4L_56",
        "outputId": "04c3db15-6aa0-489e-9b23-ee82ccd21ff9"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Already guessed': 8858,\n",
              " 'Non-alphabetic guess': 285,\n",
              " 'Guess is not a single character': 1038,\n",
              " 'Guess is not a character in word': 69,\n",
              " 'Guess empty': 168,\n",
              " 'Correct guess': 11}"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for error_type in error:\n",
        "  print(\"|\", error_type, \"|\", error[error_type], \"|\", str(round(error[error_type]/104.29, 2))+\"%\", \"|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLwhg0V9OEc1",
        "outputId": "43c2c33e-2c12-4a7c-e7f5-cdbe5872523a"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Already guessed | 8858 | 84.94% |\n",
            "| Non-alphabetic guess | 285 | 2.73% |\n",
            "| Guess is not a single character | 1038 | 9.95% |\n",
            "| Guess is not a character in word | 69 | 0.66% |\n",
            "| Guess empty | 168 | 1.61% |\n",
            "| Correct guess | 11 | 0.11% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "non_alpha_guess = {}\n",
        "for r in results:\n",
        "  if r[\"error_types\"] == ['NON_ALPHA']:\n",
        "    if r[\"guess_raw\"] not in non_alpha_guess:\n",
        "      non_alpha_guess[r[\"guess_raw\"]] = 1\n",
        "    else:\n",
        "      non_alpha_guess[r[\"guess_raw\"]] += 1"
      ],
      "metadata": {
        "id": "utlmvV6YOt2V"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_alpha_guess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qMcXmA6PIT6",
        "outputId": "64d1710a-2ece-47a9-ac9e-618e47aedb27"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_': 285}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "XHlLwnz_bnRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"You are playing a game of Hangman.\n",
        "\n",
        "Your task is to guess a single character.\n",
        "\n",
        "The word has a certain number of letters.\n",
        "The current state of the word is shown with guessed letters filled in and blanks for the unknown letters.\n",
        "The number of incorrect guesses remaining is listed.\n",
        "All letters that have been guessed so far are listed.\n",
        "\n",
        "You will format your response as a single uppercase letter at the end\n",
        "\n",
        "Here are a few examples to guide you:\n",
        "\n",
        "Example 1\n",
        "Prompt:\n",
        "The word has 5 letters.\n",
        "The current state is: A _ _ _ E\n",
        "Incorrect guesses remaining: 4\n",
        "Guessed letters: [A, E]\n",
        "Correct response: R\n",
        "\n",
        "Example 2\n",
        "Prompt:\n",
        "The word has 8 letters.\n",
        "The current state is: B A _ _ _ _ A\n",
        "Incorrect guesses remaining: 3\n",
        "Guessed letters: [B, A]\n",
        "Correct response: N\n",
        "\n",
        "Example 3\n",
        "Prompt:\n",
        "The word has 7 letters.\n",
        "The current state is: C H A _ _ _ _\n",
        "Incorrect guesses remaining: 2\n",
        "Guessed letters: [C, H, A]\n",
        "Correct response: L\n",
        "\n",
        "Example 4\n",
        "Prompt:\n",
        "The word has 6 letters.\n",
        "The current state is: _ O O _ L _\n",
        "Incorrect guesses remaining: 1\n",
        "Guessed letters: [O, L]\n",
        "Correct response: G\n",
        "\n",
        "Example 5\n",
        "\n",
        "The word has 10 letters.\n",
        "The current state is: _ _ _ A _ _ _ I _ N\n",
        "Incorrect guesses remaining: 4\n",
        "Guessed letters: [A, I, N]\n",
        "Correct response: S\n",
        "\n",
        "Now, let's play the game.\n",
        "\n",
        "The word has 6 letters.\n",
        "The current state is: P _ _ _ O _\n",
        "Incorrect guesses remaining: 5\n",
        "Guessed letters: [E, P, O]\n",
        "\n",
        "Correct response:\"\"\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **input_ids,\n",
        "    max_new_tokens=1)\n",
        "print(tokenizer.decode(outputs[0])[-20:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-WCYe8iKnwe",
        "outputId": "7eb55386-7656-4111-d755-c48545a2aa82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Correct response: R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os7VIYJIWkEx",
        "outputId": "8c8c4a93-78bb-41fc-b921-29c0e7a9c430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thinking content: <think>\n",
            "Okay, let's see. I need to figure out the correct letter to guess in this Hangman game. The word has 6 letters, and the current state is that there are four underscores, and the remaining guesses are 6. The guessed letters are empty. So the goal is to determine which letter to guess next.\n",
            "\n",
            "Wait, but the problem says \"output a single letter at the end\". Hmm, maybe I need to pick a letter that's not guessed yet, not in the wrong position, and not part of the wrong letters. Let me think.\n",
            "\n",
            "The initial state shows that there are four underscores. So the word has 6 letters, and the underscores are the letters that haven't been guessed yet. So the possible letters to guess could be any of the 6 letters that haven't been guessed yet, but also not in the wrong positions. Wait, but the problem says that the current state is \"_ _ _ _ _ _\", so maybe the underscores are the letters that need to be filled in. The remaining guesses are 6. So the user has 6 guesses left, but the word is 6 letters long. So the user is allowed to guess any of the 6 letters, but they have to choose one that hasn't been guessed yet and not in the wrong positions.\n",
            "\n",
            "Wait, but the problem says to output a single letter. So perhaps the answer is any of the letters that haven't been guessed yet. But since the guessed letters are empty, maybe any of the letters. But maybe there's a specific answer expected here. Wait, maybe the problem is a simple one where the user guesses any letter. Let me check again.\n",
            "\n",
            "The task is to guess one letter. Output a single letter. So perhaps the correct answer is any of the letters. But how to choose? Maybe the problem expects me to pick a random letter. Wait, but maybe there's a specific answer. Wait, maybe I'm overcomplicating. Let me think again.\n",
            "\n",
            "The word has 6 letters. The current state is underscores, so the letters to guess are those that are not yet guessed. The remaining guesses are 6. So the user has 6 guesses left. So they can choose any of the 6 letters, but they need to pick one. Since there's no information about the actual letters, maybe the answer is any of the letters. But how to choose? Maybe the problem expects me to pick a random one. For example, maybe the answer is 'e', but that's just a guess.\n",
            "\n",
            "Wait, but perhaps the problem is a test. Maybe the actual letters are something else. Wait, maybe there's a typo here. Let me check again. The initial state is underscores. The remaining guesses are 6. So the user can guess any of the letters. But since the problem says \"output a single letter\", maybe the answer is one of the letters. But without more information, how to choose?\n",
            "\n",
            "Wait, maybe the problem is designed such that the correct answer is any letter. For example, the user can choose any letter. So perhaps the answer is 'e'. But maybe there's a specific answer expected. Alternatively, maybe the problem is a trick question where the answer is none, but that doesn't make sense. Alternatively, maybe the answer is 'e' because it's the most common letter. Or maybe the problem is designed to have a specific answer, but without more context, it's hard to tell. \n",
            "\n",
            "Wait, maybe I'm missing something. The problem says the current state is \"_ _ _ _ _ _\", which implies that the underscores are the letters that need to be filled. The user has 6 guesses left. So the user can choose any of the 6 letters. But the problem says to output a single letter. So perhaps the answer is any letter. But how to choose? Maybe the problem expects me to pick the first letter, which is 'a', but that's a guess. \n",
            "\n",
            "Alternatively, maybe the problem is a simple one where the correct answer is 'e', as a common letter. But since the problem doesn't provide the actual letters, perhaps the answer is 'e'. \n",
            "\n",
            "Wait, maybe I should think of it as a game where the user can choose any letter, so the answer is any of the letters. But since the user hasn't been given the actual letters, perhaps the answer is 'e' as a placeholder. \n",
            "\n",
            "Alternatively, perhaps the problem is designed such that the answer is 'e', but I'm not sure. \n",
            "\n",
            "Wait, maybe I'm overcomplicating. The problem says that the user has 6 guesses left, and the current state is underscores. So the user can choose any letter. Since there's no specific instruction to choose, perhaps the answer is any letter. But the problem says to output a single letter. Maybe the answer is 'e'. \n",
            "\n",
            "But maybe there's a standard answer. For example, in some cases, the answer is 'e'. Alternatively, maybe the problem is expecting me to pick the first letter, which is 'a'. But without more information, it's impossible to know. \n",
            "\n",
            "Hmm, perhaps I need to make a best guess. Let's think again. The word has 6 letters, and the underscores are the letters that need to be guessed. The user has 6 guesses left. So they can choose any of the 6 letters. But since the problem says to output a single letter, perhaps the answer is 'e'. \n",
            "\n",
            "Alternatively, maybe the problem expects me to pick a letter that's not guessed yet. But since the guessed letters are empty, any letter would work. \n",
            "\n",
            "But since the problem says to output a single letter, perhaps the answer is 'e'. \n",
            "\n",
            "Alternatively, maybe the answer is 'a'. \n",
            "\n",
            "Wait, perhaps I should just pick a random letter. Let me think of possible letters. Since there are 6 letters, maybe the answer is 'e'. \n",
            "\n",
            "But maybe the problem is a trick. For example, the word is \"ELEPHANT\", but that's 7 letters. No, the word has 6 letters. Maybe it's \"ELEPHANT\" but that's 7. \n",
            "\n",
            "Alternatively, maybe the word is \"ELEPHANT\" but that's 7 letters. \n",
            "\n",
            "Alternatively, maybe the word is \"LEMON\". But that's 4 letters. \n",
            "\n",
            "Wait, maybe the problem is a simple one where the answer is any letter. So perhaps the answer is 'e'. \n",
            "\n",
            "But I'm not sure. Alternatively, maybe the answer is 'a'. \n",
            "\n",
            "Alternatively, perhaps the problem is a test where the answer is 'e'. \n",
            "\n",
            "Given that, I think the answer is 'e'. So the output is 'e'.\n",
            "</think>\n",
            "content: The game requires guessing one letter to complete the Hangman. Since the word has 6 letters and the current state shows underscores, the correct letter must be any of the 6 letters that haven't been guessed yet. However, without specific information about the word, the most logical and common choice (based on typical game scenarios) is to guess the letter **'e'**.\n",
            "\n",
            "Final Answer: **e**\n"
          ]
        }
      ],
      "source": [
        "# prepare the model input\n",
        "prompt = \"\"\"You are playing a game of Hangman.\n",
        "\n",
        "The word has 6 letters.\n",
        "The current state is: _ _ _ _ _ _\n",
        "Incorrect guesses remaining: 6\n",
        "Guessed letters: [ ]\n",
        "\n",
        "Your task is to guess one letter. Output a single letter at the end.\"\"\"\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=32768\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "# parsing thinking content\n",
        "try:\n",
        "    # rindex finding 151668 (</think>)\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "print(\"thinking content:\", thinking_content)\n",
        "print(\"content:\", content)\n"
      ]
    }
  ]
}