{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNERQ7ghRcWWWHIuZpaRVXy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hieunguyen7337/LLM_RL/blob/main/hangman_game_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (wandb on; bitsandbytes kept for big-model path)\n",
        "!pip -q install \"trl>=0.16.0\" transformers accelerate bitsandbytes peft wandb"
      ],
      "metadata": {
        "id": "sp27T7YMrbqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch\n",
        "from datasets import Dataset\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "sC9vPDe10H19"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Toggle: use 4-bit for big models only\n",
        "model_name   = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "ENABLE_4BIT  = False      # <- small model: False. Set True for bigger models (e.g., â‰¥7B).\n",
        "GRAD_CKPT    = ENABLE_4BIT\n",
        "USE_CACHE    = not GRAD_CKPT  # avoid the \"caching incompatible with checkpointing\" spam"
      ],
      "metadata": {
        "id": "0X2amOdu5yVY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ.setdefault(\"WANDB_PROJECT\", \"huggingface\")   # or \"grpo-demos\"\n",
        "os.environ.setdefault(\"WANDB_LOG_MODEL\", \"end\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XkFnTvh_52eN",
        "outputId": "c6d72594-e8c4-42b1-c432-f782f23f9610"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'end'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "# Define a dataset that contains both math and coding problems\n",
        "dataset = Dataset.from_list(\n",
        "    [\n",
        "        {\"prompt\": \"What is 2+2?\", \"task\": \"math\"},\n",
        "        {\"prompt\": \"Write a function that returns the sum of two numbers.\", \"task\": \"code\"},\n",
        "        {\"prompt\": \"What is 3*4?\", \"task\": \"math\"},\n",
        "        {\"prompt\": \"Write a function that returns the product of two numbers.\", \"task\": \"code\"},\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "p_9ieaZLvX5Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Reward funcs (neutral=0.0 for non-target tasks)\n",
        "# ---------------------------\n",
        "def math_reward_func(prompts, completions, task, **kwargs):\n",
        "    return [1.0 if t == \"math\" else 0.0 for t in task]\n",
        "\n",
        "def coding_reward_func(prompts, completions, task, **kwargs):\n",
        "    return [1.0 if t == \"code\" else 0.0 for t in task]"
      ],
      "metadata": {
        "id": "tg7xdtDCyufN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Tokenizer\n",
        "# ---------------------------\n",
        "tok = AutoTokenizer.from_pretrained(model_name, use_fast=True, padding_side=\"left\")\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token"
      ],
      "metadata": {
        "id": "7x1Fb30u6AiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Model (no quant for small; 4-bit kept for big)\n",
        "# ---------------------------\n",
        "quant = None\n",
        "if ENABLE_4BIT:\n",
        "    quant = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"cuda:0\" if not ENABLE_4BIT else \"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    attn_implementation=\"sdpa\",\n",
        "    use_cache=USE_CACHE,\n",
        "    quantization_config=quant,\n",
        ")"
      ],
      "metadata": {
        "id": "kQfNxWal6Dv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# LoRA (kept for both; safe with/without quant)\n",
        "# ---------------------------\n",
        "peft_cfg = LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Bb8c3qks6EhC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# GRPO config (W&B enabled; will save at epoch end)\n",
        "# ---------------------------\n",
        "args = GRPOConfig(\n",
        "    output_dir=\"qwen2.5-0.5b-grpo\",\n",
        "    per_device_train_batch_size=4,  # keep divisible by num_generations\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_generations=8,              # default is 8, batch must be divisible by this\n",
        "    max_prompt_length=128,          # default 512\n",
        "    max_completion_length=64,       # default 256\n",
        "    fp16=True,                      # T4 uses fp16\n",
        "    gradient_checkpointing=GRAD_CKPT,\n",
        "    report_to=\"wandb\",              # <-- keep W&B reporting\n",
        "    run_name=\"qwen2.5-0.5b-noquant\",  # change per run\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\",          # save at epoch end too\n",
        "    save_total_limit=2,\n",
        ")"
      ],
      "metadata": {
        "id": "2lygjrwJ6GK4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    reward_funcs=[math_reward_func, coding_reward_func],\n",
        "    train_dataset=dataset,\n",
        "    args=args,\n",
        "    peft_config=peft_cfg, # LoRA reduces trainable params & VRAM\n",
        ")"
      ],
      "metadata": {
        "id": "dWN4YY2H6H8Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vNI6xQWWOOGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "VJZ15VQ56Jhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = args.output_dir"
      ],
      "metadata": {
        "id": "JkC5ghPUCAMC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Save: adapter/weights + tokenizer + trainer state\n",
        "# ---------------------------\n",
        "trainer.save_model(save_dir)        # saves PEFT adapter (and weights) appropriately\n",
        "tok.save_pretrained(save_dir)\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "id": "fIgKew6V6ZQF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) If you're NOT using 4-bit, also export a merged FP16 model without LoRA adapters:\n",
        "if not ENABLE_4BIT:\n",
        "    try:\n",
        "        merged = trainer.model.merge_and_unload()\n",
        "        merged_dir = os.path.join(save_dir, \"merged-fp16\")\n",
        "        merged.save_pretrained(merged_dir)\n",
        "        tok.save_pretrained(merged_dir)\n",
        "        print(f\"Merged full model saved to: {merged_dir}\")\n",
        "    except Exception as e:\n",
        "        print(\"Merge skipped (not a PEFT model or unsupported):\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4G0wn4WBhgu",
        "outputId": "0a988e4f-fa4a-4803-87b4-357f75221f43"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged full model saved to: qwen2.5-0.5b-grpo/merged-fp16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# compress the folder\n",
        "shutil.make_archive(\"qwen2.5-0.5b-grpo\", 'zip', \"qwen2.5-0.5b-grpo\")\n",
        "\n",
        "# now download to your computer\n",
        "from google.colab import files\n",
        "files.download(\"qwen2.5-0.5b-grpo.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "b4SKTQGTBi7h",
        "outputId": "2b6ea46c-1d6f-44f8-db73-db0ae6760012"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f3f84d5a-52d5-47fb-b351-72dd71c06344\", \"qwen2.5-0.5b-grpo.zip\", 791392096)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}